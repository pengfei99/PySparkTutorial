{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Spark session creation\n",
    "\n",
    "In this tutorial, we will show how to create a spark session with the right configuration.\n"
   ],
   "id": "4b6c88e762a96769"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:37.774804Z",
     "start_time": "2025-11-28T13:03:37.716557Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession",
   "id": "2975e8fb5d1efc69",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:41.321812Z",
     "start_time": "2025-11-28T13:03:37.781141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalMode_memo_config\")\n",
    "    .master(\"local[*]\")\n",
    "    # enable AQE\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    # give a partition size advice.\n",
    "    .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\")\n",
    "    # set AQE partition range\n",
    "    .config(\"spark.sql.adaptive.maxNumPostShufflePartitions\", \"100\")\n",
    "    .config(\"spark.sql.adaptive.minNumPostShufflePartitions\", \"1\")\n",
    "    # to support old date like 1900-01-01\n",
    "    .config(\"spark.sql.legacy.useLegacyDateTimestamp\", \"true\")\n",
    "    # increase worker timeout\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "    .config(\"spark.sql.sources.commitProtocolClass\",\n",
    "            \"org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\")\n",
    "    # JVM memory allocation\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Half of RAM for driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "    # Shuffle & partition tuning\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "    # Unified memory management\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "    # Spill to disk early instead of crashing\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    # optimize jvm GC\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    # Use Kryo serializer\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    # Optional: buffer size for serialization\n",
    "    .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ],
   "id": "7faf33a83cee8631",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:41.380863Z",
     "start_time": "2025-11-28T13:03:41.378715Z"
    }
   },
   "cell_type": "code",
   "source": "data_folder_path = \"C:/Users/pliu/Documents/data_set/sas_vs_parquet\"",
   "id": "1214e01c3b336ddd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:41.390602Z",
     "start_time": "2025-11-28T13:03:41.388398Z"
    }
   },
   "cell_type": "code",
   "source": "parquet_file1= f\"{data_folder_path}/nyc_taxi_1GB_parquet.parquet\"",
   "id": "7a22221c362703aa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:44.332656Z",
     "start_time": "2025-11-28T13:03:41.397541Z"
    }
   },
   "cell_type": "code",
   "source": "df1 = spark.read.parquet(parquet_file1)",
   "id": "b0a53ff775f6ee38",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T13:03:45.086612Z",
     "start_time": "2025-11-28T13:03:44.380392Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"partition number is : {df1.rdd.getNumPartitions()}\")",
   "id": "5a0ebca416489383",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition number is : 18\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:53:34.247792Z",
     "start_time": "2025-11-28T12:53:31.275471Z"
    }
   },
   "cell_type": "code",
   "source": "df1.show(5)",
   "id": "f206af3c28bb0d6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|vendor_id|          pickup_at|         dropoff_at|passenger_count|trip_distance|pickup_longitude|pickup_latitude|rate_code_id|store_and_fwd_flag|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "|      VTS|2009-01-04 03:52:00|2009-01-04 04:02:00|              1|            2|             -73|             40|        NULL|              NULL|              -73|              40|        CASH|          8|    0|   NULL|         0|           0|           9|\n",
      "|      VTS|2009-01-04 04:31:00|2009-01-04 04:38:00|              3|            4|             -73|             40|        NULL|              NULL|              -73|              40|      Credit|         12|    0|   NULL|         2|           0|          14|\n",
      "|      VTS|2009-01-03 16:43:00|2009-01-03 16:57:00|              5|           10|             -74|             40|        NULL|              NULL|              -73|              40|      Credit|         23|    0|   NULL|         4|           0|          28|\n",
      "|      DDS|2009-01-01 21:52:58|2009-01-01 22:14:00|              1|            5|             -73|             40|        NULL|              NULL|              -73|              40|      CREDIT|         14|    0|   NULL|         3|           0|          18|\n",
      "|      DDS|2009-01-24 17:18:23|2009-01-24 17:24:56|              1|            0|             -74|             40|        NULL|              NULL|              -74|              40|        CASH|          3|    0|   NULL|         0|           0|           3|\n",
      "+---------+-------------------+-------------------+---------------+-------------+----------------+---------------+------------+------------------+-----------------+----------------+------------+-----------+-----+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:55:33.583212Z",
     "start_time": "2025-11-28T12:55:33.560323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "different_passenger_count = df1.groupBy(\"passenger_count\").agg(count(\"passenger_count\").alias(\"count_of_diff_passengers\"))"
   ],
   "id": "85d373fac42bc0cc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T12:55:39.657237Z",
     "start_time": "2025-11-28T12:55:38.342674Z"
    }
   },
   "cell_type": "code",
   "source": "different_passenger_count.show(5)",
   "id": "ac8527df7cd41557",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------------+\n",
      "|passenger_count|count_of_diff_passengers|\n",
      "+---------------+------------------------+\n",
      "|              0|                     377|\n",
      "|              6|                   36200|\n",
      "|              5|                  719399|\n",
      "|              1|                 5359229|\n",
      "|              3|                  353516|\n",
      "+---------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ec49af7fdaa44431"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
